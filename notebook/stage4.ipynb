{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38KDeZero': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d635527afea6bfbfe0fe92e0ba5306b59308b18132c20aeb255a24cdcf85f20c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from kdezero import Variable, Model\n",
    "from kdezero.utils import plot_dot_graph\n",
    "import kdezero.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import kdezero.layers as L\n",
    "from kdezero import optimizers\n",
    "from kdezero.models import MLP\n",
    "from kdezero import datasets\n",
    "import math\n",
    "import kdezero\n",
    "from kdezero import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[1 1 1]\n          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.reshape(x, (6,))\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[[-0.44496268 -0.71000492 -0.0781721 ]\n           [ 0.7889931  -1.31355772  0.39044899]]])\nvariable([[-0.44496268 -0.71000492 -0.0781721 ]\n          [ 0.7889931  -1.31355772  0.39044899]])\nvariable([[-0.44496268 -0.71000492 -0.0781721 ]\n          [ 0.7889931  -1.31355772  0.39044899]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.random.randn(1, 2, 3))\n",
    "print(x)\n",
    "y = x.reshape((2, 3))\n",
    "print(y)\n",
    "y = x.reshape(2, 3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[1 1 1]\n          [1 1 1]])\nvariable([[1 1]\n          [1 1]\n          [1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.transpose(x)\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[1 1 1]\n          [1 1 1]])\nvariable([[1 1]\n          [1 1]\n          [1 1]])\nvariable([[1 1 1]\n          [1 1 1]])\nvariable([[1 1]\n          [1 1]\n          [1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = x.transpose()\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = x.T\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[[[0.67896932 0.788375   0.51260571 0.60458275]\n            [0.70414355 0.0767171  0.18420816 0.27504341]\n            [0.00416768 0.55407867 0.18721799 0.77130623]]\n         \n           [[0.54663742 0.60545831 0.97274262 0.82604792]\n            [0.11557495 0.74382158 0.60566996 0.67324778]\n            [0.09215439 0.59391866 0.5173335  0.42114142]]]])\nvariable([[[[1. 1. 1. 1.]\n            [1. 1. 1. 1.]\n            [1. 1. 1. 1.]]\n         \n           [[1. 1. 1. 1.]\n            [1. 1. 1. 1.]\n            [1. 1. 1. 1.]]]])\nvariable([[[[1. 1. 1.]\n            [1. 1. 1.]\n            [1. 1. 1.]\n            [1. 1. 1.]]]\n         \n         \n          [[[1. 1. 1.]\n            [1. 1. 1.]\n            [1. 1. 1.]\n            [1. 1. 1.]]]])\nvariable([[[[0.67896932 0.70414355 0.00416768]\n            [0.788375   0.0767171  0.55407867]\n            [0.51260571 0.18420816 0.18721799]\n            [0.60458275 0.27504341 0.77130623]]]\n         \n         \n          [[[0.54663742 0.11557495 0.09215439]\n            [0.60545831 0.74382158 0.59391866]\n            [0.97274262 0.60566996 0.5173335 ]\n            [0.82604792 0.67324778 0.42114142]]]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.random.rand(1, 2, 3, 4))\n",
    "print(x)\n",
    "y = x.transpose(1, 0, 3, 2)\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([0.1 0.2 0.3])\nvariable([-0.06])\n"
     ]
    }
   ],
   "source": [
    "x0 = Variable(np.array([1, 2, 3]))\n",
    "x1 = Variable(np.array([10]))\n",
    "y = x0 / x1\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(21)\nvariable([1 1 1 1 1 1])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([1, 2, 3, 4, 5, 6]))\n",
    "y = F.sum(x)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(21)\nvariable([[1 1 1]\n          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.sum(x)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([5 7 9])\nvariable([[1 1 1]\n          [1 1 1]])\n(1, 1, 1, 1)\nvariable([[[[1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]]\n         \n           [[1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]]\n         \n           [[1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]]]\n         \n         \n          [[[1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]]\n         \n           [[1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]]\n         \n           [[1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]\n            [1. 1. 1. 1. 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.sum(x, axis=0)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)\n",
    "\n",
    "x = Variable(np.random.randn(2, 3, 4, 5))\n",
    "y = x.sum(keepdims=True)\n",
    "y.backward()\n",
    "print(y.shape)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 3)\n(3, 4)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.random.randn(2, 3))\n",
    "W = Variable(np.random.randn(3, 4))\n",
    "y = F.matmul(x, W)\n",
    "y.backward()\n",
    "\n",
    "print(x.grad.shape)\n",
    "print(W.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[0.64433458]]) variable([1.29473389]) variable(42.296340129442335)\nvariable([[1.12672345]]) variable([2.26959351]) variable(23.97380754378544)\nvariable([[1.48734571]]) variable([3.00386712]) variable(13.609686745040522)\nvariable([[1.75641886]]) variable([3.557186]) variable(7.747049961219976)\nvariable([[1.95666851]]) variable([3.97439789]) variable(4.43057410592155)\nvariable([[2.10518573]]) variable([4.28923203]) variable(2.554280381353593)\nvariable([[2.21482401]]) variable([4.52705574]) variable(1.492599869047195)\nvariable([[2.29524981]]) variable([4.70694745]) variable(0.8916952181756939)\nvariable([[2.35373273]]) variable([4.84325585]) variable(0.5514270962227455)\nvariable([[2.39573972]]) variable([4.9467725]) variable(0.3585915308319281)\nvariable([[2.425382]]) variable([5.02561369]) variable(0.24915731977561134)\nvariable([[2.44575118]]) variable([5.08588371]) variable(0.1869065876539789)\nvariable([[2.45917205]]) variable([5.13217364]) variable(0.1513533629631488)\nvariable([[2.4673927]]) variable([5.16793652]) variable(0.13091003006317087)\nvariable([[2.47172747]]) variable([5.19576949]) variable(0.11902210735018467)\nvariable([[2.47316455]]) variable([5.21762597]) variable(0.11198198322254362)\nvariable([[2.47244676]]) variable([5.23497527]) variable(0.10769231158094321)\nvariable([[2.47013247]]) variable([5.24892259]) variable(0.10496655795675108)\nvariable([[2.46664127]]) variable([5.26029927]) variable(0.10313337115761934)\nvariable([[2.46228843]]) variable([5.26973075]) variable(0.10181280604960243)\nvariable([[2.45731071]]) variable([5.27768752]) variable(0.10078974954301652)\nvariable([[2.4518859]]) variable([5.28452363]) variable(0.09994232708821599)\nvariable([[2.44614738]]) variable([5.29050548]) variable(0.09920140749444821)\nvariable([[2.44019517]]) variable([5.29583359]) variable(0.09852769772358984)\nvariable([[2.4341042]]) variable([5.30065891]) variable(0.09789878700703991)\nvariable([[2.4279305]]) variable([5.30509512]) variable(0.09730181854646197)\nvariable([[2.42171596]]) variable([5.30922787]) variable(0.0967293443169877)\nvariable([[2.41549177]]) variable([5.3131217]) variable(0.09617698031441604)\nvariable([[2.40928112]]) variable([5.31682532]) variable(0.09564208018092028)\nvariable([[2.40310116]]) variable([5.32037549]) variable(0.09512298485383605)\nvariable([[2.39696452]]) variable([5.3238]) variable(0.09461859803040694)\nvariable([[2.39088043]]) variable([5.32711987]) variable(0.09412814592514404)\nvariable([[2.38485555]]) variable([5.33035108]) variable(0.09365104127065577)\nvariable([[2.37889464]]) variable([5.33350575]) variable(0.09318680628411545)\nvariable([[2.37300101]]) variable([5.33659314]) variable(0.09273502898904006)\nvariable([[2.3671769]]) variable([5.33962035]) variable(0.09229533840647547)\nvariable([[2.36142374]]) variable([5.34259283]) variable(0.09186739042193233)\nvariable([[2.35574235]]) variable([5.34551483]) variable(0.09145085969346782)\nvariable([[2.35013309]]) variable([5.34838966]) variable(0.09104543497939387)\nvariable([[2.34459602]]) variable([5.35121993]) variable(0.09065081640275062)\nvariable([[2.33913091]]) variable([5.35400772]) variable(0.0902667138137311)\nvariable([[2.33373736]]) variable([5.35675472]) variable(0.08989284577554084)\nvariable([[2.32841486]]) variable([5.35946234]) variable(0.0895289389052284)\nvariable([[2.32316275]]) variable([5.36213172]) variable(0.08917472741757472)\nvariable([[2.31798034]]) variable([5.36476385]) variable(0.08882995278605695)\nvariable([[2.31286689]]) variable([5.3673596]) variable(0.08849436347219049)\nvariable([[2.30782159]]) variable([5.36991973]) variable(0.08816771469564999)\nvariable([[2.30284363]]) variable([5.3724449]) variable(0.08784976822950144)\nvariable([[2.29793221]]) variable([5.37493575]) variable(0.08754029221162851)\nvariable([[2.29308646]]) variable([5.37739285]) variable(0.08723906096725743)\nvariable([[2.28830557]]) variable([5.37981674]) variable(0.08694585483964615)\nvariable([[2.2835887]]) variable([5.38220792]) variable(0.0866604600272269)\nvariable([[2.278935]]) variable([5.38456689]) variable(0.08638266842618712)\nvariable([[2.27434366]]) variable([5.38689411]) variable(0.0861122774778659)\nvariable([[2.26981384]]) variable([5.38919004]) variable(0.08584909002056745)\nvariable([[2.26534474]]) variable([5.39145512]) variable(0.08559291414552149)\nvariable([[2.26093555]]) variable([5.39368978]) variable(0.08534356305679344)\nvariable([[2.25658547]]) variable([5.39589443]) variable(0.08510085493499035)\nvariable([[2.25229371]]) variable([5.39806949]) variable(0.08486461280463413)\nvariable([[2.2480595]]) variable([5.40021536]) variable(0.08463466440508784)\nvariable([[2.24388206]]) variable([5.40233244]) variable(0.08441084206493275)\nvariable([[2.23976064]]) variable([5.40442111]) variable(0.08419298257969864)\nvariable([[2.23569448]]) variable([5.40648177]) variable(0.08398092709285435)\nvariable([[2.23168285]]) variable([5.40851479]) variable(0.08377452097997208)\nvariable([[2.22772501]]) variable([5.41052054]) variable(0.08357361373597812)\nvariable([[2.22382025]]) variable([5.41249938]) variable(0.08337805886540826)\nvariable([[2.21996785]]) variable([5.41445169]) variable(0.08318771377558704)\nvariable([[2.21616712]]) variable([5.41637782]) variable(0.08300243967265336)\nvariable([[2.21241735]]) variable([5.41827811]) variable(0.08282210146035615)\nvariable([[2.20871786]]) variable([5.42015291]) variable(0.08264656764154595)\nvariable([[2.20506799]]) variable([5.42200258]) variable(0.08247571022229121)\nvariable([[2.20146706]]) variable([5.42382744]) variable(0.08230940461854906)\nvariable([[2.19791443]]) variable([5.42562782]) variable(0.08214752956532231)\nvariable([[2.19440943]]) variable([5.42740407]) variable(0.08198996702823673)\nvariable([[2.19095144]]) variable([5.42915649]) variable(0.08183660211747391)\nvariable([[2.18753983]]) variable([5.43088541]) variable(0.08168732300399718)\nvariable([[2.18417396]]) variable([5.43259114]) variable(0.08154202083800904)\nvariable([[2.18085323]]) variable([5.434274]) variable(0.08140058966958151)\nvariable([[2.17757703]]) variable([5.4359343]) variable(0.08126292637140048)\nvariable([[2.17434477]]) variable([5.43757232]) variable(0.0811289305635685)\nvariable([[2.17115586]]) variable([5.43918838]) variable(0.08099850454041051)\nvariable([[2.16800971]]) variable([5.44078277]) variable(0.0808715531992303)\nvariable([[2.16490575]]) variable([5.44235578]) variable(0.08074798397096412)\nvariable([[2.16184341]]) variable([5.44390769]) variable(0.08062770675268231)\nvariable([[2.15882214]]) variable([5.44543879]) variable(0.08051063384188899)\nvariable([[2.15584139]]) variable([5.44694936]) variable(0.08039667987257197)\nvariable([[2.15290062]]) variable([5.44843967]) variable(0.08028576175295649)\nvariable([[2.14999928]]) variable([5.44990999]) variable(0.08017779860491725)\nvariable([[2.14713684]]) variable([5.4513606]) variable(0.08007271170500452)\nvariable([[2.1443128]]) variable([5.45279175]) variable(0.07997042442704135)\nvariable([[2.14152662]]) variable([5.45420371]) variable(0.07987086218625004)\nvariable([[2.13877781]]) variable([5.45559674]) variable(0.07977395238486742)\nvariable([[2.13606587]]) variable([5.45697108]) variable(0.07967962435920853)\nvariable([[2.13339029]]) variable([5.458327]) variable(0.07958780932814088)\nvariable([[2.13075059]]) variable([5.45966473]) variable(0.07949844034293135)\nvariable([[2.12814629]]) variable([5.46098452]) variable(0.07941145223842926)\nvariable([[2.12557692]]) variable([5.46228661]) variable(0.07932678158554966)\nvariable([[2.123042]]) variable([5.46357124]) variable(0.07924436664502324)\nvariable([[2.12054108]]) variable([5.46483864]) variable(0.07916414732237737)\nvariable([[2.11807369]]) variable([5.46608905]) variable(0.07908606512411756)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 5 + 2 * x + np.random.rand(100, 1)\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "W = Variable(np.zeros((1, 1)))\n",
    "b = Variable(np.zeros(1))\n",
    "\n",
    "def predict(x):\n",
    "    y = F.matmul(x, W) + b\n",
    "    return y\n",
    "\n",
    "# def mean_squared_error(x0, x1):\n",
    "#     diff = x0 - x1\n",
    "#     return F.sum(diff ** 2) / len(diff)\n",
    "\n",
    "lr = 0.1\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    W.cleargrad()\n",
    "    b.cleargrad()\n",
    "    loss.backward()\n",
    "\n",
    "    W.data -= lr * W.grad.data\n",
    "    b.data -= lr * b.grad.data\n",
    "    print(W, b, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(0.8473695850105871)\n",
      "variable(0.2514286285183606)\n",
      "variable(0.2475948546674987)\n",
      "variable(0.23786120447054826)\n",
      "variable(0.21222231333102934)\n",
      "variable(0.16742181117834185)\n",
      "variable(0.09681932619992686)\n",
      "variable(0.07849528290602335)\n",
      "variable(0.07749729552991157)\n",
      "variable(0.0772213239955932)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "I, H, O = 1, 10, 1\n",
    "W1 = Variable(0.01 * np.random.randn(I, H))\n",
    "b1 = Variable(np.zeros(H))\n",
    "W2 = Variable(0.01 * np.random.randn(H, O))\n",
    "b2 = Variable(np.zeros(O))\n",
    "\n",
    "def predict(x):\n",
    "    y = F.linear(x, W1, b1)\n",
    "    y = F.sigmoid(y)\n",
    "    y = F.linear(y, W2, b2)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    W1.cleargrad()\n",
    "    b1.cleargrad()\n",
    "    W2.cleargrad()\n",
    "    b2.cleargrad()\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data -= lr * W1.grad.data\n",
    "    b1.data -= lr * b1.grad.data\n",
    "    W2.data -= lr * W2.grad.data\n",
    "    b2.data -= lr * b2.grad.data\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649353)\n",
      "variable(0.0788816650635515)\n",
      "variable(0.07655073683421636)\n",
      "variable(0.0763780308623822)\n",
      "variable(0.07618764131185572)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "l1 = L.Linear(10)\n",
    "l2 = L.Linear(1)\n",
    "\n",
    "def predict(x):\n",
    "    y = l1(x)\n",
    "    y = F.sigmoid(y)\n",
    "    y = l2(y)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    l1.cleargrads()\n",
    "    l2.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    for l in [l1, l2]:\n",
    "        for p in l.params():\n",
    "            p.data -= lr * p.grad.data\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "class TowLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "\n",
    "x = Variable(np.random.randn(5, 10), name='x')\n",
    "model = TowLayerNet(100, 10)\n",
    "model.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649353)\n",
      "variable(0.0788816650635515)\n",
      "variable(0.07655073683421636)\n",
      "variable(0.0763780308623822)\n",
      "variable(0.07618764131185572)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "class TowLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "\n",
    "model = TowLayerNet(hidden_size, 1)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.params():\n",
    "        p.data -= lr * p.grad.data\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649353)\n",
      "variable(0.0788816650635515)\n",
      "variable(0.07655073683421636)\n",
      "variable(0.0763780308623822)\n",
      "variable(0.07618764131185572)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.SGD(lr)\n",
    "optimizer.setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.update()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.07743134827996008)\n",
      "variable(0.07544895146731473)\n",
      "variable(0.07463260305858642)\n",
      "variable(0.07420983776361516)\n",
      "variable(0.07397000396385318)\n",
      "variable(0.07383179319278564)\n",
      "variable(0.07375198316276851)\n",
      "variable(0.07370578149495666)\n",
      "variable(0.07367887538341851)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.MomentumSGD(lr)\n",
    "optimizer.setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.update()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([4 5 6])\nvariable([[0. 0. 0.]\n          [1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.get_item(x, 1)\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[1 2 3]\n          [1 2 3]\n          [4 5 6]])\nvariable([[2. 2. 2.]\n          [1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.get_item(x, [0, 0, 1])\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([4 5 6])\nvariable([[0. 0. 0.]\n          [1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = x[1]\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [6]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "x.max(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[-0.09907545  0.80404522  0.25896994]\n          [ 0.03931895  0.59199818  0.26970795]\n          [-0.39121382  1.2116953   0.31731228]\n          [-0.11999959  0.82979856  0.44505524]])\nvariable(1.1921845964066193)\n"
     ]
    }
   ],
   "source": [
    "model = MLP((10, 3))\n",
    "\n",
    "x = np.array([[0.2, -0.4], [0.3, 0.5], [1.3, -3.2], [2.1, 0.3]])\n",
    "t = np.array([2, 0, 1, 0])\n",
    "y = model(x)\n",
    "print(y)\n",
    "\n",
    "loss = F.softmax_cross_entropy(y, t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(300, 2)\n(300,)\n[-0.12995958 -0.00324155] 1\n[ 0.3282343  -0.54941994] 0\n"
     ]
    }
   ],
   "source": [
    "x, t = datasets.get_spiral(train=True)\n",
    "print(x.shape)\n",
    "print(t.shape)\n",
    "\n",
    "print(x[10], t[10])\n",
    "print(x[110], t[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, lo 1.13\n",
      "epoch 2, lo 1.05\n",
      "epoch 3, lo 0.95\n",
      "epoch 4, lo 0.92\n",
      "epoch 5, lo 0.87\n",
      "epoch 6, lo 0.89\n",
      "epoch 7, lo 0.84\n",
      "epoch 8, lo 0.78\n",
      "epoch 9, lo 0.80\n",
      "epoch 10, lo 0.79\n",
      "epoch 11, lo 0.78\n",
      "epoch 12, lo 0.76\n",
      "epoch 13, lo 0.77\n",
      "epoch 14, lo 0.76\n",
      "epoch 15, lo 0.76\n",
      "epoch 16, lo 0.77\n",
      "epoch 17, lo 0.78\n",
      "epoch 18, lo 0.74\n",
      "epoch 19, lo 0.74\n",
      "epoch 20, lo 0.72\n",
      "epoch 21, lo 0.73\n",
      "epoch 22, lo 0.74\n",
      "epoch 23, lo 0.77\n",
      "epoch 24, lo 0.73\n",
      "epoch 25, lo 0.74\n",
      "epoch 26, lo 0.74\n",
      "epoch 27, lo 0.72\n",
      "epoch 28, lo 0.72\n",
      "epoch 29, lo 0.72\n",
      "epoch 30, lo 0.73\n",
      "epoch 31, lo 0.71\n",
      "epoch 32, lo 0.72\n",
      "epoch 33, lo 0.72\n",
      "epoch 34, lo 0.71\n",
      "epoch 35, lo 0.72\n",
      "epoch 36, lo 0.71\n",
      "epoch 37, lo 0.71\n",
      "epoch 38, lo 0.70\n",
      "epoch 39, lo 0.71\n",
      "epoch 40, lo 0.70\n",
      "epoch 41, lo 0.71\n",
      "epoch 42, lo 0.70\n",
      "epoch 43, lo 0.70\n",
      "epoch 44, lo 0.70\n",
      "epoch 45, lo 0.69\n",
      "epoch 46, lo 0.69\n",
      "epoch 47, lo 0.71\n",
      "epoch 48, lo 0.70\n",
      "epoch 49, lo 0.69\n",
      "epoch 50, lo 0.69\n",
      "epoch 51, lo 0.68\n",
      "epoch 52, lo 0.67\n",
      "epoch 53, lo 0.68\n",
      "epoch 54, lo 0.70\n",
      "epoch 55, lo 0.68\n",
      "epoch 56, lo 0.66\n",
      "epoch 57, lo 0.67\n",
      "epoch 58, lo 0.66\n",
      "epoch 59, lo 0.64\n",
      "epoch 60, lo 0.64\n",
      "epoch 61, lo 0.64\n",
      "epoch 62, lo 0.63\n",
      "epoch 63, lo 0.63\n",
      "epoch 64, lo 0.61\n",
      "epoch 65, lo 0.61\n",
      "epoch 66, lo 0.60\n",
      "epoch 67, lo 0.62\n",
      "epoch 68, lo 0.59\n",
      "epoch 69, lo 0.60\n",
      "epoch 70, lo 0.57\n",
      "epoch 71, lo 0.58\n",
      "epoch 72, lo 0.57\n",
      "epoch 73, lo 0.56\n",
      "epoch 74, lo 0.56\n",
      "epoch 75, lo 0.55\n",
      "epoch 76, lo 0.55\n",
      "epoch 77, lo 0.55\n",
      "epoch 78, lo 0.54\n",
      "epoch 79, lo 0.53\n",
      "epoch 80, lo 0.53\n",
      "epoch 81, lo 0.52\n",
      "epoch 82, lo 0.53\n",
      "epoch 83, lo 0.52\n",
      "epoch 84, lo 0.49\n",
      "epoch 85, lo 0.50\n",
      "epoch 86, lo 0.49\n",
      "epoch 87, lo 0.49\n",
      "epoch 88, lo 0.48\n",
      "epoch 89, lo 0.47\n",
      "epoch 90, lo 0.47\n",
      "epoch 91, lo 0.46\n",
      "epoch 92, lo 0.46\n",
      "epoch 93, lo 0.45\n",
      "epoch 94, lo 0.44\n",
      "epoch 95, lo 0.45\n",
      "epoch 96, lo 0.44\n",
      "epoch 97, lo 0.43\n",
      "epoch 98, lo 0.43\n",
      "epoch 99, lo 0.42\n",
      "epoch 100, lo 0.43\n",
      "epoch 101, lo 0.42\n",
      "epoch 102, lo 0.41\n",
      "epoch 103, lo 0.42\n",
      "epoch 104, lo 0.41\n",
      "epoch 105, lo 0.40\n",
      "epoch 106, lo 0.40\n",
      "epoch 107, lo 0.40\n",
      "epoch 108, lo 0.39\n",
      "epoch 109, lo 0.38\n",
      "epoch 110, lo 0.39\n",
      "epoch 111, lo 0.38\n",
      "epoch 112, lo 0.38\n",
      "epoch 113, lo 0.38\n",
      "epoch 114, lo 0.36\n",
      "epoch 115, lo 0.36\n",
      "epoch 116, lo 0.36\n",
      "epoch 117, lo 0.36\n",
      "epoch 118, lo 0.36\n",
      "epoch 119, lo 0.35\n",
      "epoch 120, lo 0.35\n",
      "epoch 121, lo 0.36\n",
      "epoch 122, lo 0.34\n",
      "epoch 123, lo 0.35\n",
      "epoch 124, lo 0.33\n",
      "epoch 125, lo 0.33\n",
      "epoch 126, lo 0.32\n",
      "epoch 127, lo 0.34\n",
      "epoch 128, lo 0.32\n",
      "epoch 129, lo 0.33\n",
      "epoch 130, lo 0.31\n",
      "epoch 131, lo 0.30\n",
      "epoch 132, lo 0.31\n",
      "epoch 133, lo 0.31\n",
      "epoch 134, lo 0.30\n",
      "epoch 135, lo 0.29\n",
      "epoch 136, lo 0.29\n",
      "epoch 137, lo 0.29\n",
      "epoch 138, lo 0.28\n",
      "epoch 139, lo 0.29\n",
      "epoch 140, lo 0.28\n",
      "epoch 141, lo 0.27\n",
      "epoch 142, lo 0.27\n",
      "epoch 143, lo 0.28\n",
      "epoch 144, lo 0.27\n",
      "epoch 145, lo 0.27\n",
      "epoch 146, lo 0.26\n",
      "epoch 147, lo 0.26\n",
      "epoch 148, lo 0.26\n",
      "epoch 149, lo 0.26\n",
      "epoch 150, lo 0.25\n",
      "epoch 151, lo 0.25\n",
      "epoch 152, lo 0.25\n",
      "epoch 153, lo 0.24\n",
      "epoch 154, lo 0.24\n",
      "epoch 155, lo 0.24\n",
      "epoch 156, lo 0.24\n",
      "epoch 157, lo 0.24\n",
      "epoch 158, lo 0.24\n",
      "epoch 159, lo 0.23\n",
      "epoch 160, lo 0.23\n",
      "epoch 161, lo 0.23\n",
      "epoch 162, lo 0.23\n",
      "epoch 163, lo 0.23\n",
      "epoch 164, lo 0.22\n",
      "epoch 165, lo 0.22\n",
      "epoch 166, lo 0.22\n",
      "epoch 167, lo 0.21\n",
      "epoch 168, lo 0.22\n",
      "epoch 169, lo 0.22\n",
      "epoch 170, lo 0.21\n",
      "epoch 171, lo 0.21\n",
      "epoch 172, lo 0.22\n",
      "epoch 173, lo 0.22\n",
      "epoch 174, lo 0.21\n",
      "epoch 175, lo 0.21\n",
      "epoch 176, lo 0.20\n",
      "epoch 177, lo 0.21\n",
      "epoch 178, lo 0.20\n",
      "epoch 179, lo 0.20\n",
      "epoch 180, lo 0.20\n",
      "epoch 181, lo 0.20\n",
      "epoch 182, lo 0.19\n",
      "epoch 183, lo 0.20\n",
      "epoch 184, lo 0.19\n",
      "epoch 185, lo 0.19\n",
      "epoch 186, lo 0.19\n",
      "epoch 187, lo 0.19\n",
      "epoch 188, lo 0.19\n",
      "epoch 189, lo 0.19\n",
      "epoch 190, lo 0.19\n",
      "epoch 191, lo 0.19\n",
      "epoch 192, lo 0.19\n",
      "epoch 193, lo 0.18\n",
      "epoch 194, lo 0.19\n",
      "epoch 195, lo 0.18\n",
      "epoch 196, lo 0.18\n",
      "epoch 197, lo 0.18\n",
      "epoch 198, lo 0.18\n",
      "epoch 199, lo 0.19\n",
      "epoch 200, lo 0.18\n",
      "epoch 201, lo 0.17\n",
      "epoch 202, lo 0.18\n",
      "epoch 203, lo 0.18\n",
      "epoch 204, lo 0.17\n",
      "epoch 205, lo 0.18\n",
      "epoch 206, lo 0.17\n",
      "epoch 207, lo 0.17\n",
      "epoch 208, lo 0.17\n",
      "epoch 209, lo 0.17\n",
      "epoch 210, lo 0.17\n",
      "epoch 211, lo 0.17\n",
      "epoch 212, lo 0.17\n",
      "epoch 213, lo 0.18\n",
      "epoch 214, lo 0.17\n",
      "epoch 215, lo 0.17\n",
      "epoch 216, lo 0.17\n",
      "epoch 217, lo 0.17\n",
      "epoch 218, lo 0.17\n",
      "epoch 219, lo 0.16\n",
      "epoch 220, lo 0.17\n",
      "epoch 221, lo 0.16\n",
      "epoch 222, lo 0.16\n",
      "epoch 223, lo 0.16\n",
      "epoch 224, lo 0.16\n",
      "epoch 225, lo 0.16\n",
      "epoch 226, lo 0.16\n",
      "epoch 227, lo 0.17\n",
      "epoch 228, lo 0.18\n",
      "epoch 229, lo 0.16\n",
      "epoch 230, lo 0.16\n",
      "epoch 231, lo 0.15\n",
      "epoch 232, lo 0.16\n",
      "epoch 233, lo 0.17\n",
      "epoch 234, lo 0.16\n",
      "epoch 235, lo 0.16\n",
      "epoch 236, lo 0.15\n",
      "epoch 237, lo 0.16\n",
      "epoch 238, lo 0.16\n",
      "epoch 239, lo 0.16\n",
      "epoch 240, lo 0.16\n",
      "epoch 241, lo 0.15\n",
      "epoch 242, lo 0.15\n",
      "epoch 243, lo 0.15\n",
      "epoch 244, lo 0.15\n",
      "epoch 245, lo 0.15\n",
      "epoch 246, lo 0.15\n",
      "epoch 247, lo 0.15\n",
      "epoch 248, lo 0.15\n",
      "epoch 249, lo 0.15\n",
      "epoch 250, lo 0.15\n",
      "epoch 251, lo 0.15\n",
      "epoch 252, lo 0.15\n",
      "epoch 253, lo 0.15\n",
      "epoch 254, lo 0.15\n",
      "epoch 255, lo 0.15\n",
      "epoch 256, lo 0.15\n",
      "epoch 257, lo 0.14\n",
      "epoch 258, lo 0.15\n",
      "epoch 259, lo 0.14\n",
      "epoch 260, lo 0.15\n",
      "epoch 261, lo 0.15\n",
      "epoch 262, lo 0.15\n",
      "epoch 263, lo 0.14\n",
      "epoch 264, lo 0.14\n",
      "epoch 265, lo 0.14\n",
      "epoch 266, lo 0.14\n",
      "epoch 267, lo 0.14\n",
      "epoch 268, lo 0.14\n",
      "epoch 269, lo 0.14\n",
      "epoch 270, lo 0.14\n",
      "epoch 271, lo 0.14\n",
      "epoch 272, lo 0.14\n",
      "epoch 273, lo 0.14\n",
      "epoch 274, lo 0.14\n",
      "epoch 275, lo 0.14\n",
      "epoch 276, lo 0.14\n",
      "epoch 277, lo 0.14\n",
      "epoch 278, lo 0.14\n",
      "epoch 279, lo 0.14\n",
      "epoch 280, lo 0.13\n",
      "epoch 281, lo 0.13\n",
      "epoch 282, lo 0.14\n",
      "epoch 283, lo 0.13\n",
      "epoch 284, lo 0.13\n",
      "epoch 285, lo 0.13\n",
      "epoch 286, lo 0.13\n",
      "epoch 287, lo 0.14\n",
      "epoch 288, lo 0.13\n",
      "epoch 289, lo 0.13\n",
      "epoch 290, lo 0.13\n",
      "epoch 291, lo 0.13\n",
      "epoch 292, lo 0.13\n",
      "epoch 293, lo 0.14\n",
      "epoch 294, lo 0.13\n",
      "epoch 295, lo 0.13\n",
      "epoch 296, lo 0.13\n",
      "epoch 297, lo 0.13\n",
      "epoch 298, lo 0.12\n",
      "epoch 299, lo 0.13\n",
      "epoch 300, lo 0.13\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "x, t = datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        batch_index = index[i * batch_size: (i + 1) *  batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, lo %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.12995958 -0.00324155] 1 variable([-4.0917854   3.2415352   0.11706614])\n[ 0.3282343  -0.54941994] 0 variable([ 4.560401  -6.4638186  1.0734416])\n"
     ]
    }
   ],
   "source": [
    "print(x[10], t[10], model(x[10]))\n",
    "print(x[110], t[110], model(x[110]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(array([-0.13981389, -0.00721657], dtype=float32), 1)\n300\n"
     ]
    }
   ],
   "source": [
    "train_set = kdezero.datasets.Spiral(train=True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, lo 1.13\n",
      "epoch 2, lo 1.05\n",
      "epoch 3, lo 0.95\n",
      "epoch 4, lo 0.92\n",
      "epoch 5, lo 0.87\n",
      "epoch 6, lo 0.89\n",
      "epoch 7, lo 0.84\n",
      "epoch 8, lo 0.78\n",
      "epoch 9, lo 0.80\n",
      "epoch 10, lo 0.79\n",
      "epoch 11, lo 0.78\n",
      "epoch 12, lo 0.76\n",
      "epoch 13, lo 0.77\n",
      "epoch 14, lo 0.76\n",
      "epoch 15, lo 0.76\n",
      "epoch 16, lo 0.77\n",
      "epoch 17, lo 0.78\n",
      "epoch 18, lo 0.74\n",
      "epoch 19, lo 0.74\n",
      "epoch 20, lo 0.72\n",
      "epoch 21, lo 0.73\n",
      "epoch 22, lo 0.74\n",
      "epoch 23, lo 0.77\n",
      "epoch 24, lo 0.73\n",
      "epoch 25, lo 0.74\n",
      "epoch 26, lo 0.74\n",
      "epoch 27, lo 0.72\n",
      "epoch 28, lo 0.72\n",
      "epoch 29, lo 0.72\n",
      "epoch 30, lo 0.73\n",
      "epoch 31, lo 0.71\n",
      "epoch 32, lo 0.72\n",
      "epoch 33, lo 0.72\n",
      "epoch 34, lo 0.71\n",
      "epoch 35, lo 0.72\n",
      "epoch 36, lo 0.71\n",
      "epoch 37, lo 0.71\n",
      "epoch 38, lo 0.70\n",
      "epoch 39, lo 0.71\n",
      "epoch 40, lo 0.70\n",
      "epoch 41, lo 0.71\n",
      "epoch 42, lo 0.70\n",
      "epoch 43, lo 0.70\n",
      "epoch 44, lo 0.70\n",
      "epoch 45, lo 0.69\n",
      "epoch 46, lo 0.69\n",
      "epoch 47, lo 0.71\n",
      "epoch 48, lo 0.70\n",
      "epoch 49, lo 0.69\n",
      "epoch 50, lo 0.69\n",
      "epoch 51, lo 0.68\n",
      "epoch 52, lo 0.67\n",
      "epoch 53, lo 0.68\n",
      "epoch 54, lo 0.70\n",
      "epoch 55, lo 0.68\n",
      "epoch 56, lo 0.66\n",
      "epoch 57, lo 0.67\n",
      "epoch 58, lo 0.66\n",
      "epoch 59, lo 0.64\n",
      "epoch 60, lo 0.64\n",
      "epoch 61, lo 0.64\n",
      "epoch 62, lo 0.63\n",
      "epoch 63, lo 0.63\n",
      "epoch 64, lo 0.61\n",
      "epoch 65, lo 0.61\n",
      "epoch 66, lo 0.60\n",
      "epoch 67, lo 0.62\n",
      "epoch 68, lo 0.59\n",
      "epoch 69, lo 0.60\n",
      "epoch 70, lo 0.57\n",
      "epoch 71, lo 0.58\n",
      "epoch 72, lo 0.57\n",
      "epoch 73, lo 0.56\n",
      "epoch 74, lo 0.56\n",
      "epoch 75, lo 0.55\n",
      "epoch 76, lo 0.55\n",
      "epoch 77, lo 0.55\n",
      "epoch 78, lo 0.54\n",
      "epoch 79, lo 0.53\n",
      "epoch 80, lo 0.53\n",
      "epoch 81, lo 0.52\n",
      "epoch 82, lo 0.53\n",
      "epoch 83, lo 0.52\n",
      "epoch 84, lo 0.49\n",
      "epoch 85, lo 0.50\n",
      "epoch 86, lo 0.49\n",
      "epoch 87, lo 0.49\n",
      "epoch 88, lo 0.48\n",
      "epoch 89, lo 0.47\n",
      "epoch 90, lo 0.47\n",
      "epoch 91, lo 0.46\n",
      "epoch 92, lo 0.46\n",
      "epoch 93, lo 0.45\n",
      "epoch 94, lo 0.44\n",
      "epoch 95, lo 0.45\n",
      "epoch 96, lo 0.44\n",
      "epoch 97, lo 0.43\n",
      "epoch 98, lo 0.43\n",
      "epoch 99, lo 0.42\n",
      "epoch 100, lo 0.43\n",
      "epoch 101, lo 0.42\n",
      "epoch 102, lo 0.41\n",
      "epoch 103, lo 0.42\n",
      "epoch 104, lo 0.41\n",
      "epoch 105, lo 0.40\n",
      "epoch 106, lo 0.40\n",
      "epoch 107, lo 0.40\n",
      "epoch 108, lo 0.39\n",
      "epoch 109, lo 0.38\n",
      "epoch 110, lo 0.39\n",
      "epoch 111, lo 0.38\n",
      "epoch 112, lo 0.38\n",
      "epoch 113, lo 0.38\n",
      "epoch 114, lo 0.36\n",
      "epoch 115, lo 0.36\n",
      "epoch 116, lo 0.36\n",
      "epoch 117, lo 0.36\n",
      "epoch 118, lo 0.36\n",
      "epoch 119, lo 0.35\n",
      "epoch 120, lo 0.35\n",
      "epoch 121, lo 0.36\n",
      "epoch 122, lo 0.34\n",
      "epoch 123, lo 0.35\n",
      "epoch 124, lo 0.33\n",
      "epoch 125, lo 0.33\n",
      "epoch 126, lo 0.32\n",
      "epoch 127, lo 0.34\n",
      "epoch 128, lo 0.32\n",
      "epoch 129, lo 0.33\n",
      "epoch 130, lo 0.31\n",
      "epoch 131, lo 0.30\n",
      "epoch 132, lo 0.31\n",
      "epoch 133, lo 0.31\n",
      "epoch 134, lo 0.30\n",
      "epoch 135, lo 0.29\n",
      "epoch 136, lo 0.29\n",
      "epoch 137, lo 0.29\n",
      "epoch 138, lo 0.28\n",
      "epoch 139, lo 0.29\n",
      "epoch 140, lo 0.28\n",
      "epoch 141, lo 0.27\n",
      "epoch 142, lo 0.27\n",
      "epoch 143, lo 0.28\n",
      "epoch 144, lo 0.27\n",
      "epoch 145, lo 0.27\n",
      "epoch 146, lo 0.26\n",
      "epoch 147, lo 0.26\n",
      "epoch 148, lo 0.26\n",
      "epoch 149, lo 0.26\n",
      "epoch 150, lo 0.25\n",
      "epoch 151, lo 0.25\n",
      "epoch 152, lo 0.25\n",
      "epoch 153, lo 0.24\n",
      "epoch 154, lo 0.24\n",
      "epoch 155, lo 0.24\n",
      "epoch 156, lo 0.24\n",
      "epoch 157, lo 0.24\n",
      "epoch 158, lo 0.24\n",
      "epoch 159, lo 0.23\n",
      "epoch 160, lo 0.23\n",
      "epoch 161, lo 0.23\n",
      "epoch 162, lo 0.23\n",
      "epoch 163, lo 0.23\n",
      "epoch 164, lo 0.22\n",
      "epoch 165, lo 0.22\n",
      "epoch 166, lo 0.22\n",
      "epoch 167, lo 0.21\n",
      "epoch 168, lo 0.22\n",
      "epoch 169, lo 0.22\n",
      "epoch 170, lo 0.21\n",
      "epoch 171, lo 0.21\n",
      "epoch 172, lo 0.22\n",
      "epoch 173, lo 0.22\n",
      "epoch 174, lo 0.21\n",
      "epoch 175, lo 0.21\n",
      "epoch 176, lo 0.20\n",
      "epoch 177, lo 0.21\n",
      "epoch 178, lo 0.20\n",
      "epoch 179, lo 0.20\n",
      "epoch 180, lo 0.20\n",
      "epoch 181, lo 0.20\n",
      "epoch 182, lo 0.19\n",
      "epoch 183, lo 0.20\n",
      "epoch 184, lo 0.19\n",
      "epoch 185, lo 0.19\n",
      "epoch 186, lo 0.19\n",
      "epoch 187, lo 0.19\n",
      "epoch 188, lo 0.19\n",
      "epoch 189, lo 0.19\n",
      "epoch 190, lo 0.19\n",
      "epoch 191, lo 0.19\n",
      "epoch 192, lo 0.19\n",
      "epoch 193, lo 0.18\n",
      "epoch 194, lo 0.19\n",
      "epoch 195, lo 0.18\n",
      "epoch 196, lo 0.18\n",
      "epoch 197, lo 0.18\n",
      "epoch 198, lo 0.18\n",
      "epoch 199, lo 0.19\n",
      "epoch 200, lo 0.18\n",
      "epoch 201, lo 0.17\n",
      "epoch 202, lo 0.18\n",
      "epoch 203, lo 0.18\n",
      "epoch 204, lo 0.17\n",
      "epoch 205, lo 0.18\n",
      "epoch 206, lo 0.17\n",
      "epoch 207, lo 0.17\n",
      "epoch 208, lo 0.17\n",
      "epoch 209, lo 0.17\n",
      "epoch 210, lo 0.17\n",
      "epoch 211, lo 0.17\n",
      "epoch 212, lo 0.17\n",
      "epoch 213, lo 0.18\n",
      "epoch 214, lo 0.17\n",
      "epoch 215, lo 0.17\n",
      "epoch 216, lo 0.17\n",
      "epoch 217, lo 0.17\n",
      "epoch 218, lo 0.17\n",
      "epoch 219, lo 0.16\n",
      "epoch 220, lo 0.17\n",
      "epoch 221, lo 0.16\n",
      "epoch 222, lo 0.16\n",
      "epoch 223, lo 0.16\n",
      "epoch 224, lo 0.16\n",
      "epoch 225, lo 0.16\n",
      "epoch 226, lo 0.16\n",
      "epoch 227, lo 0.17\n",
      "epoch 228, lo 0.18\n",
      "epoch 229, lo 0.16\n",
      "epoch 230, lo 0.16\n",
      "epoch 231, lo 0.15\n",
      "epoch 232, lo 0.16\n",
      "epoch 233, lo 0.17\n",
      "epoch 234, lo 0.16\n",
      "epoch 235, lo 0.16\n",
      "epoch 236, lo 0.15\n",
      "epoch 237, lo 0.16\n",
      "epoch 238, lo 0.16\n",
      "epoch 239, lo 0.16\n",
      "epoch 240, lo 0.16\n",
      "epoch 241, lo 0.15\n",
      "epoch 242, lo 0.15\n",
      "epoch 243, lo 0.15\n",
      "epoch 244, lo 0.15\n",
      "epoch 245, lo 0.15\n",
      "epoch 246, lo 0.15\n",
      "epoch 247, lo 0.15\n",
      "epoch 248, lo 0.15\n",
      "epoch 249, lo 0.15\n",
      "epoch 250, lo 0.15\n",
      "epoch 251, lo 0.15\n",
      "epoch 252, lo 0.15\n",
      "epoch 253, lo 0.15\n",
      "epoch 254, lo 0.15\n",
      "epoch 255, lo 0.15\n",
      "epoch 256, lo 0.15\n",
      "epoch 257, lo 0.14\n",
      "epoch 258, lo 0.15\n",
      "epoch 259, lo 0.14\n",
      "epoch 260, lo 0.15\n",
      "epoch 261, lo 0.15\n",
      "epoch 262, lo 0.15\n",
      "epoch 263, lo 0.14\n",
      "epoch 264, lo 0.14\n",
      "epoch 265, lo 0.14\n",
      "epoch 266, lo 0.14\n",
      "epoch 267, lo 0.14\n",
      "epoch 268, lo 0.14\n",
      "epoch 269, lo 0.14\n",
      "epoch 270, lo 0.14\n",
      "epoch 271, lo 0.14\n",
      "epoch 272, lo 0.14\n",
      "epoch 273, lo 0.14\n",
      "epoch 274, lo 0.14\n",
      "epoch 275, lo 0.14\n",
      "epoch 276, lo 0.14\n",
      "epoch 277, lo 0.14\n",
      "epoch 278, lo 0.14\n",
      "epoch 279, lo 0.14\n",
      "epoch 280, lo 0.13\n",
      "epoch 281, lo 0.13\n",
      "epoch 282, lo 0.14\n",
      "epoch 283, lo 0.13\n",
      "epoch 284, lo 0.13\n",
      "epoch 285, lo 0.13\n",
      "epoch 286, lo 0.13\n",
      "epoch 287, lo 0.14\n",
      "epoch 288, lo 0.13\n",
      "epoch 289, lo 0.13\n",
      "epoch 290, lo 0.13\n",
      "epoch 291, lo 0.13\n",
      "epoch 292, lo 0.13\n",
      "epoch 293, lo 0.14\n",
      "epoch 294, lo 0.13\n",
      "epoch 295, lo 0.13\n",
      "epoch 296, lo 0.13\n",
      "epoch 297, lo 0.13\n",
      "epoch 298, lo 0.12\n",
      "epoch 299, lo 0.13\n",
      "epoch 300, lo 0.13\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = datasets.Spiral()\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        batch_index = index[i * batch_size: (i + 1) *  batch_size]\n",
    "        batch = [train_set[i] for i in batch_index]\n",
    "        batch_x = np.array([exsample[0] for exsample in batch])\n",
    "        batch_t = np.array([exsample[1] for exsample in batch])\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, lo %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 2) (10,)\n(10, 2) (10,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "max_epoch = 1\n",
    "\n",
    "train_set = datasets.Spiral(train=True)\n",
    "test_set = datasets.Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for x, t in train_loader:\n",
    "        print(x.shape, t.shape)\n",
    "        break\n",
    "\n",
    "    for x, t in test_loader:\n",
    "        print(x.shape, t.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ": 0.6667\n",
      "test loss: 0.6076, accuracy: 0.6967\n",
      "epoch: 63\n",
      "train loss: 0.5914, accuracy: 0.6700\n",
      "test loss: 0.6020, accuracy: 0.6300\n",
      "epoch: 64\n",
      "train loss: 0.5911, accuracy: 0.6867\n",
      "test loss: 0.5981, accuracy: 0.6533\n",
      "epoch: 65\n",
      "train loss: 0.5626, accuracy: 0.7000\n",
      "test loss: 0.6124, accuracy: 0.7100\n",
      "epoch: 66\n",
      "train loss: 0.5707, accuracy: 0.7067\n",
      "test loss: 0.5760, accuracy: 0.6567\n",
      "epoch: 67\n",
      "train loss: 0.5574, accuracy: 0.6900\n",
      "test loss: 0.5834, accuracy: 0.6967\n",
      "epoch: 68\n",
      "train loss: 0.5560, accuracy: 0.7133\n",
      "test loss: 0.5589, accuracy: 0.6933\n",
      "epoch: 69\n",
      "train loss: 0.5355, accuracy: 0.7267\n",
      "test loss: 0.5530, accuracy: 0.6967\n",
      "epoch: 70\n",
      "train loss: 0.5303, accuracy: 0.7300\n",
      "test loss: 0.5380, accuracy: 0.7267\n",
      "epoch: 71\n",
      "train loss: 0.5207, accuracy: 0.7167\n",
      "test loss: 0.5315, accuracy: 0.7367\n",
      "epoch: 72\n",
      "train loss: 0.5094, accuracy: 0.7533\n",
      "test loss: 0.5340, accuracy: 0.7367\n",
      "epoch: 73\n",
      "train loss: 0.5110, accuracy: 0.7400\n",
      "test loss: 0.5127, accuracy: 0.7433\n",
      "epoch: 74\n",
      "train loss: 0.4921, accuracy: 0.7500\n",
      "test loss: 0.5051, accuracy: 0.7400\n",
      "epoch: 75\n",
      "train loss: 0.4882, accuracy: 0.7700\n",
      "test loss: 0.5268, accuracy: 0.7467\n",
      "epoch: 76\n",
      "train loss: 0.4859, accuracy: 0.7533\n",
      "test loss: 0.4997, accuracy: 0.7400\n",
      "epoch: 77\n",
      "train loss: 0.4701, accuracy: 0.7767\n",
      "test loss: 0.4814, accuracy: 0.7800\n",
      "epoch: 78\n",
      "train loss: 0.4574, accuracy: 0.7733\n",
      "test loss: 0.4736, accuracy: 0.7700\n",
      "epoch: 79\n",
      "train loss: 0.4547, accuracy: 0.7533\n",
      "test loss: 0.4665, accuracy: 0.7900\n",
      "epoch: 80\n",
      "train loss: 0.4533, accuracy: 0.7667\n",
      "test loss: 0.4601, accuracy: 0.7900\n",
      "epoch: 81\n",
      "train loss: 0.4431, accuracy: 0.7967\n",
      "test loss: 0.4496, accuracy: 0.7900\n",
      "epoch: 82\n",
      "train loss: 0.4309, accuracy: 0.8167\n",
      "test loss: 0.4421, accuracy: 0.7867\n",
      "epoch: 83\n",
      "train loss: 0.4282, accuracy: 0.8167\n",
      "test loss: 0.4431, accuracy: 0.7833\n",
      "epoch: 84\n",
      "train loss: 0.4132, accuracy: 0.8167\n",
      "test loss: 0.4353, accuracy: 0.8200\n",
      "epoch: 85\n",
      "train loss: 0.4132, accuracy: 0.8167\n",
      "test loss: 0.4260, accuracy: 0.8367\n",
      "epoch: 86\n",
      "train loss: 0.4047, accuracy: 0.8167\n",
      "test loss: 0.4248, accuracy: 0.8400\n",
      "epoch: 87\n",
      "train loss: 0.3968, accuracy: 0.8367\n",
      "test loss: 0.4031, accuracy: 0.8333\n",
      "epoch: 88\n",
      "train loss: 0.3942, accuracy: 0.8267\n",
      "test loss: 0.4091, accuracy: 0.8333\n",
      "epoch: 89\n",
      "train loss: 0.3746, accuracy: 0.8333\n",
      "test loss: 0.3962, accuracy: 0.8100\n",
      "epoch: 90\n",
      "train loss: 0.3772, accuracy: 0.8333\n",
      "test loss: 0.3831, accuracy: 0.8300\n",
      "epoch: 91\n",
      "train loss: 0.3669, accuracy: 0.8467\n",
      "test loss: 0.3844, accuracy: 0.8433\n",
      "epoch: 92\n",
      "train loss: 0.3554, accuracy: 0.8600\n",
      "test loss: 0.3752, accuracy: 0.8567\n",
      "epoch: 93\n",
      "train loss: 0.3491, accuracy: 0.8700\n",
      "test loss: 0.3720, accuracy: 0.8400\n",
      "epoch: 94\n",
      "train loss: 0.3440, accuracy: 0.8533\n",
      "test loss: 0.3595, accuracy: 0.8600\n",
      "epoch: 95\n",
      "train loss: 0.3408, accuracy: 0.8633\n",
      "test loss: 0.3533, accuracy: 0.8700\n",
      "epoch: 96\n",
      "train loss: 0.3336, accuracy: 0.8600\n",
      "test loss: 0.3515, accuracy: 0.8800\n",
      "epoch: 97\n",
      "train loss: 0.3295, accuracy: 0.8700\n",
      "test loss: 0.3437, accuracy: 0.8933\n",
      "epoch: 98\n",
      "train loss: 0.3208, accuracy: 0.9133\n",
      "test loss: 0.3433, accuracy: 0.8533\n",
      "epoch: 99\n",
      "train loss: 0.3212, accuracy: 0.8667\n",
      "test loss: 0.3302, accuracy: 0.8633\n",
      "epoch: 100\n",
      "train loss: 0.3146, accuracy: 0.8700\n",
      "test loss: 0.3248, accuracy: 0.8600\n",
      "epoch: 101\n",
      "train loss: 0.3090, accuracy: 0.8633\n",
      "test loss: 0.3268, accuracy: 0.8833\n",
      "epoch: 102\n",
      "train loss: 0.3024, accuracy: 0.8933\n",
      "test loss: 0.3195, accuracy: 0.8533\n",
      "epoch: 103\n",
      "train loss: 0.3009, accuracy: 0.8933\n",
      "test loss: 0.3122, accuracy: 0.8900\n",
      "epoch: 104\n",
      "train loss: 0.2930, accuracy: 0.9067\n",
      "test loss: 0.3087, accuracy: 0.8633\n",
      "epoch: 105\n",
      "train loss: 0.2881, accuracy: 0.9067\n",
      "test loss: 0.3111, accuracy: 0.8567\n",
      "epoch: 106\n",
      "train loss: 0.2785, accuracy: 0.9167\n",
      "test loss: 0.3026, accuracy: 0.8867\n",
      "epoch: 107\n",
      "train loss: 0.2786, accuracy: 0.8933\n",
      "test loss: 0.2952, accuracy: 0.8833\n",
      "epoch: 108\n",
      "train loss: 0.2756, accuracy: 0.9100\n",
      "test loss: 0.2912, accuracy: 0.8800\n",
      "epoch: 109\n",
      "train loss: 0.2744, accuracy: 0.9200\n",
      "test loss: 0.2965, accuracy: 0.8667\n",
      "epoch: 110\n",
      "train loss: 0.2705, accuracy: 0.9267\n",
      "test loss: 0.2856, accuracy: 0.8933\n",
      "epoch: 111\n",
      "train loss: 0.2645, accuracy: 0.9067\n",
      "test loss: 0.2814, accuracy: 0.8867\n",
      "epoch: 112\n",
      "train loss: 0.2624, accuracy: 0.9100\n",
      "test loss: 0.2835, accuracy: 0.8700\n",
      "epoch: 113\n",
      "train loss: 0.2608, accuracy: 0.9067\n",
      "test loss: 0.2752, accuracy: 0.8933\n",
      "epoch: 114\n",
      "train loss: 0.2524, accuracy: 0.9167\n",
      "test loss: 0.2740, accuracy: 0.9033\n",
      "epoch: 115\n",
      "train loss: 0.2484, accuracy: 0.9167\n",
      "test loss: 0.2688, accuracy: 0.8967\n",
      "epoch: 116\n",
      "train loss: 0.2480, accuracy: 0.9200\n",
      "test loss: 0.2768, accuracy: 0.8600\n",
      "epoch: 117\n",
      "train loss: 0.2445, accuracy: 0.9133\n",
      "test loss: 0.2635, accuracy: 0.9067\n",
      "epoch: 118\n",
      "train loss: 0.2444, accuracy: 0.9267\n",
      "test loss: 0.2593, accuracy: 0.8967\n",
      "epoch: 119\n",
      "train loss: 0.2379, accuracy: 0.9267\n",
      "test loss: 0.2576, accuracy: 0.8867\n",
      "epoch: 120\n",
      "train loss: 0.2361, accuracy: 0.9200\n",
      "test loss: 0.2573, accuracy: 0.9200\n",
      "epoch: 121\n",
      "train loss: 0.2372, accuracy: 0.9333\n",
      "test loss: 0.2534, accuracy: 0.9067\n",
      "epoch: 122\n",
      "train loss: 0.2305, accuracy: 0.9333\n",
      "test loss: 0.2511, accuracy: 0.8867\n",
      "epoch: 123\n",
      "train loss: 0.2277, accuracy: 0.9367\n",
      "test loss: 0.2531, accuracy: 0.9000\n",
      "epoch: 124\n",
      "train loss: 0.2318, accuracy: 0.9233\n",
      "test loss: 0.2456, accuracy: 0.9067\n",
      "epoch: 125\n",
      "train loss: 0.2248, accuracy: 0.9400\n",
      "test loss: 0.2454, accuracy: 0.9133\n",
      "epoch: 126\n",
      "train loss: 0.2229, accuracy: 0.9433\n",
      "test loss: 0.2427, accuracy: 0.9033\n",
      "epoch: 127\n",
      "train loss: 0.2264, accuracy: 0.9167\n",
      "test loss: 0.2433, accuracy: 0.9000\n",
      "epoch: 128\n",
      "train loss: 0.2152, accuracy: 0.9333\n",
      "test loss: 0.2373, accuracy: 0.9133\n",
      "epoch: 129\n",
      "train loss: 0.2176, accuracy: 0.9367\n",
      "test loss: 0.2402, accuracy: 0.8967\n",
      "epoch: 130\n",
      "train loss: 0.2149, accuracy: 0.9200\n",
      "test loss: 0.2332, accuracy: 0.9033\n",
      "epoch: 131\n",
      "train loss: 0.2136, accuracy: 0.9333\n",
      "test loss: 0.2377, accuracy: 0.8933\n",
      "epoch: 132\n",
      "train loss: 0.2143, accuracy: 0.9200\n",
      "test loss: 0.2304, accuracy: 0.9167\n",
      "epoch: 133\n",
      "train loss: 0.2141, accuracy: 0.9267\n",
      "test loss: 0.2449, accuracy: 0.8833\n",
      "epoch: 134\n",
      "train loss: 0.2088, accuracy: 0.9133\n",
      "test loss: 0.2362, accuracy: 0.8900\n",
      "epoch: 135\n",
      "train loss: 0.2046, accuracy: 0.9333\n",
      "test loss: 0.2278, accuracy: 0.9133\n",
      "epoch: 136\n",
      "train loss: 0.2021, accuracy: 0.9333\n",
      "test loss: 0.2275, accuracy: 0.9200\n",
      "epoch: 137\n",
      "train loss: 0.2046, accuracy: 0.9333\n",
      "test loss: 0.2236, accuracy: 0.9167\n",
      "epoch: 138\n",
      "train loss: 0.1991, accuracy: 0.9267\n",
      "test loss: 0.2207, accuracy: 0.9233\n",
      "epoch: 139\n",
      "train loss: 0.1960, accuracy: 0.9433\n",
      "test loss: 0.2216, accuracy: 0.9267\n",
      "epoch: 140\n",
      "train loss: 0.1981, accuracy: 0.9333\n",
      "test loss: 0.2178, accuracy: 0.9233\n",
      "epoch: 141\n",
      "train loss: 0.1945, accuracy: 0.9367\n",
      "test loss: 0.2194, accuracy: 0.9200\n",
      "epoch: 142\n",
      "train loss: 0.1969, accuracy: 0.9233\n",
      "test loss: 0.2158, accuracy: 0.9233\n",
      "epoch: 143\n",
      "train loss: 0.1942, accuracy: 0.9333\n",
      "test loss: 0.2127, accuracy: 0.9167\n",
      "epoch: 144\n",
      "train loss: 0.1886, accuracy: 0.9433\n",
      "test loss: 0.2288, accuracy: 0.8933\n",
      "epoch: 145\n",
      "train loss: 0.1966, accuracy: 0.9133\n",
      "test loss: 0.2103, accuracy: 0.9167\n",
      "epoch: 146\n",
      "train loss: 0.1899, accuracy: 0.9300\n",
      "test loss: 0.2124, accuracy: 0.9100\n",
      "epoch: 147\n",
      "train loss: 0.1890, accuracy: 0.9400\n",
      "test loss: 0.2104, accuracy: 0.9200\n",
      "epoch: 148\n",
      "train loss: 0.1865, accuracy: 0.9333\n",
      "test loss: 0.2080, accuracy: 0.9267\n",
      "epoch: 149\n",
      "train loss: 0.1825, accuracy: 0.9367\n",
      "test loss: 0.2285, accuracy: 0.8967\n",
      "epoch: 150\n",
      "train loss: 0.1890, accuracy: 0.9233\n",
      "test loss: 0.2152, accuracy: 0.9067\n",
      "epoch: 151\n",
      "train loss: 0.1828, accuracy: 0.9400\n",
      "test loss: 0.2042, accuracy: 0.9300\n",
      "epoch: 152\n",
      "train loss: 0.1807, accuracy: 0.9467\n",
      "test loss: 0.2136, accuracy: 0.9067\n",
      "epoch: 153\n",
      "train loss: 0.1795, accuracy: 0.9333\n",
      "test loss: 0.2076, accuracy: 0.9033\n",
      "epoch: 154\n",
      "train loss: 0.1797, accuracy: 0.9300\n",
      "test loss: 0.2052, accuracy: 0.9200\n",
      "epoch: 155\n",
      "train loss: 0.1816, accuracy: 0.9400\n",
      "test loss: 0.2015, accuracy: 0.9233\n",
      "epoch: 156\n",
      "train loss: 0.1791, accuracy: 0.9300\n",
      "test loss: 0.2012, accuracy: 0.9333\n",
      "epoch: 157\n",
      "train loss: 0.1791, accuracy: 0.9367\n",
      "test loss: 0.1978, accuracy: 0.9267\n",
      "epoch: 158\n",
      "train loss: 0.1751, accuracy: 0.9400\n",
      "test loss: 0.2046, accuracy: 0.9033\n",
      "epoch: 159\n",
      "train loss: 0.1719, accuracy: 0.9433\n",
      "test loss: 0.2039, accuracy: 0.9067\n",
      "epoch: 160\n",
      "train loss: 0.1730, accuracy: 0.9433\n",
      "test loss: 0.1963, accuracy: 0.9300\n",
      "epoch: 161\n",
      "train loss: 0.1690, accuracy: 0.9467\n",
      "test loss: 0.1973, accuracy: 0.9267\n",
      "epoch: 162\n",
      "train loss: 0.1676, accuracy: 0.9567\n",
      "test loss: 0.1951, accuracy: 0.9400\n",
      "epoch: 163\n",
      "train loss: 0.1734, accuracy: 0.9433\n",
      "test loss: 0.1941, accuracy: 0.9300\n",
      "epoch: 164\n",
      "train loss: 0.1725, accuracy: 0.9233\n",
      "test loss: 0.1922, accuracy: 0.9367\n",
      "epoch: 165\n",
      "train loss: 0.1665, accuracy: 0.9433\n",
      "test loss: 0.1951, accuracy: 0.9233\n",
      "epoch: 166\n",
      "train loss: 0.1698, accuracy: 0.9433\n",
      "test loss: 0.1907, accuracy: 0.9333\n",
      "epoch: 167\n",
      "train loss: 0.1737, accuracy: 0.9300\n",
      "test loss: 0.1923, accuracy: 0.9300\n",
      "epoch: 168\n",
      "train loss: 0.1661, accuracy: 0.9367\n",
      "test loss: 0.1944, accuracy: 0.9300\n",
      "epoch: 169\n",
      "train loss: 0.1635, accuracy: 0.9567\n",
      "test loss: 0.1894, accuracy: 0.9233\n",
      "epoch: 170\n",
      "train loss: 0.1688, accuracy: 0.9400\n",
      "test loss: 0.1878, accuracy: 0.9300\n",
      "epoch: 171\n",
      "train loss: 0.1675, accuracy: 0.9300\n",
      "test loss: 0.1899, accuracy: 0.9367\n",
      "epoch: 172\n",
      "train loss: 0.1602, accuracy: 0.9500\n",
      "test loss: 0.1859, accuracy: 0.9400\n",
      "epoch: 173\n",
      "train loss: 0.1644, accuracy: 0.9433\n",
      "test loss: 0.1901, accuracy: 0.9267\n",
      "epoch: 174\n",
      "train loss: 0.1597, accuracy: 0.9500\n",
      "test loss: 0.1878, accuracy: 0.9267\n",
      "epoch: 175\n",
      "train loss: 0.1599, accuracy: 0.9400\n",
      "test loss: 0.1840, accuracy: 0.9367\n",
      "epoch: 176\n",
      "train loss: 0.1612, accuracy: 0.9533\n",
      "test loss: 0.1909, accuracy: 0.9200\n",
      "epoch: 177\n",
      "train loss: 0.1582, accuracy: 0.9500\n",
      "test loss: 0.1829, accuracy: 0.9433\n",
      "epoch: 178\n",
      "train loss: 0.1599, accuracy: 0.9433\n",
      "test loss: 0.1847, accuracy: 0.9333\n",
      "epoch: 179\n",
      "train loss: 0.1541, accuracy: 0.9400\n",
      "test loss: 0.1892, accuracy: 0.9267\n",
      "epoch: 180\n",
      "train loss: 0.1539, accuracy: 0.9500\n",
      "test loss: 0.1814, accuracy: 0.9367\n",
      "epoch: 181\n",
      "train loss: 0.1572, accuracy: 0.9400\n",
      "test loss: 0.1960, accuracy: 0.9333\n",
      "epoch: 182\n",
      "train loss: 0.1612, accuracy: 0.9400\n",
      "test loss: 0.1824, accuracy: 0.9367\n",
      "epoch: 183\n",
      "train loss: 0.1533, accuracy: 0.9533\n",
      "test loss: 0.1836, accuracy: 0.9233\n",
      "epoch: 184\n",
      "train loss: 0.1526, accuracy: 0.9533\n",
      "test loss: 0.1787, accuracy: 0.9433\n",
      "epoch: 185\n",
      "train loss: 0.1530, accuracy: 0.9533\n",
      "test loss: 0.1892, accuracy: 0.9100\n",
      "epoch: 186\n",
      "train loss: 0.1558, accuracy: 0.9433\n",
      "test loss: 0.1840, accuracy: 0.9267\n",
      "epoch: 187\n",
      "train loss: 0.1559, accuracy: 0.9400\n",
      "test loss: 0.1775, accuracy: 0.9367\n",
      "epoch: 188\n",
      "train loss: 0.1513, accuracy: 0.9500\n",
      "test loss: 0.1775, accuracy: 0.9367\n",
      "epoch: 189\n",
      "train loss: 0.1491, accuracy: 0.9533\n",
      "test loss: 0.1850, accuracy: 0.9267\n",
      "epoch: 190\n",
      "train loss: 0.1488, accuracy: 0.9500\n",
      "test loss: 0.1790, accuracy: 0.9333\n",
      "epoch: 191\n",
      "train loss: 0.1499, accuracy: 0.9467\n",
      "test loss: 0.1755, accuracy: 0.9400\n",
      "epoch: 192\n",
      "train loss: 0.1469, accuracy: 0.9433\n",
      "test loss: 0.1788, accuracy: 0.9267\n",
      "epoch: 193\n",
      "train loss: 0.1528, accuracy: 0.9433\n",
      "test loss: 0.1769, accuracy: 0.9367\n",
      "epoch: 194\n",
      "train loss: 0.1482, accuracy: 0.9433\n",
      "test loss: 0.1737, accuracy: 0.9467\n",
      "epoch: 195\n",
      "train loss: 0.1425, accuracy: 0.9533\n",
      "test loss: 0.1770, accuracy: 0.9333\n",
      "epoch: 196\n",
      "train loss: 0.1493, accuracy: 0.9433\n",
      "test loss: 0.1757, accuracy: 0.9300\n",
      "epoch: 197\n",
      "train loss: 0.1436, accuracy: 0.9467\n",
      "test loss: 0.1752, accuracy: 0.9300\n",
      "epoch: 198\n",
      "train loss: 0.1449, accuracy: 0.9433\n",
      "test loss: 0.1721, accuracy: 0.9433\n",
      "epoch: 199\n",
      "train loss: 0.1456, accuracy: 0.9467\n",
      "test loss: 0.1734, accuracy: 0.9300\n",
      "epoch: 200\n",
      "train loss: 0.1447, accuracy: 0.9500\n",
      "test loss: 0.1710, accuracy: 0.9433\n",
      "epoch: 201\n",
      "train loss: 0.1426, accuracy: 0.9500\n",
      "test loss: 0.1707, accuracy: 0.9433\n",
      "epoch: 202\n",
      "train loss: 0.1406, accuracy: 0.9600\n",
      "test loss: 0.1733, accuracy: 0.9400\n",
      "epoch: 203\n",
      "train loss: 0.1388, accuracy: 0.9667\n",
      "test loss: 0.1760, accuracy: 0.9333\n",
      "epoch: 204\n",
      "train loss: 0.1427, accuracy: 0.9433\n",
      "test loss: 0.1696, accuracy: 0.9467\n",
      "epoch: 205\n",
      "train loss: 0.1403, accuracy: 0.9567\n",
      "test loss: 0.1754, accuracy: 0.9433\n",
      "epoch: 206\n",
      "train loss: 0.1436, accuracy: 0.9400\n",
      "test loss: 0.1698, accuracy: 0.9433\n",
      "epoch: 207\n",
      "train loss: 0.1410, accuracy: 0.9567\n",
      "test loss: 0.1723, accuracy: 0.9333\n",
      "epoch: 208\n",
      "train loss: 0.1418, accuracy: 0.9567\n",
      "test loss: 0.1685, accuracy: 0.9400\n",
      "epoch: 209\n",
      "train loss: 0.1408, accuracy: 0.9467\n",
      "test loss: 0.1695, accuracy: 0.9367\n",
      "epoch: 210\n",
      "train loss: 0.1382, accuracy: 0.9533\n",
      "test loss: 0.1676, accuracy: 0.9467\n",
      "epoch: 211\n",
      "train loss: 0.1359, accuracy: 0.9633\n",
      "test loss: 0.1815, accuracy: 0.9367\n",
      "epoch: 212\n",
      "train loss: 0.1377, accuracy: 0.9600\n",
      "test loss: 0.1677, accuracy: 0.9433\n",
      "epoch: 213\n",
      "train loss: 0.1350, accuracy: 0.9467\n",
      "test loss: 0.1669, accuracy: 0.9467\n",
      "epoch: 214\n",
      "train loss: 0.1340, accuracy: 0.9600\n",
      "test loss: 0.1727, accuracy: 0.9367\n",
      "epoch: 215\n",
      "train loss: 0.1425, accuracy: 0.9567\n",
      "test loss: 0.1654, accuracy: 0.9467\n",
      "epoch: 216\n",
      "train loss: 0.1404, accuracy: 0.9533\n",
      "test loss: 0.1685, accuracy: 0.9333\n",
      "epoch: 217\n",
      "train loss: 0.1349, accuracy: 0.9567\n",
      "test loss: 0.1688, accuracy: 0.9367\n",
      "epoch: 218\n",
      "train loss: 0.1410, accuracy: 0.9533\n",
      "test loss: 0.1678, accuracy: 0.9300\n",
      "epoch: 219\n",
      "train loss: 0.1342, accuracy: 0.9533\n",
      "test loss: 0.1663, accuracy: 0.9400\n",
      "epoch: 220\n",
      "train loss: 0.1361, accuracy: 0.9500\n",
      "test loss: 0.1649, accuracy: 0.9433\n",
      "epoch: 221\n",
      "train loss: 0.1361, accuracy: 0.9500\n",
      "test loss: 0.1632, accuracy: 0.9533\n",
      "epoch: 222\n",
      "train loss: 0.1318, accuracy: 0.9633\n",
      "test loss: 0.1661, accuracy: 0.9433\n",
      "epoch: 223\n",
      "train loss: 0.1345, accuracy: 0.9567\n",
      "test loss: 0.1629, accuracy: 0.9467\n",
      "epoch: 224\n",
      "train loss: 0.1319, accuracy: 0.9600\n",
      "test loss: 0.1629, accuracy: 0.9533\n",
      "epoch: 225\n",
      "train loss: 0.1334, accuracy: 0.9533\n",
      "test loss: 0.1620, accuracy: 0.9500\n",
      "epoch: 226\n",
      "train loss: 0.1339, accuracy: 0.9633\n",
      "test loss: 0.1675, accuracy: 0.9367\n",
      "epoch: 227\n",
      "train loss: 0.1364, accuracy: 0.9467\n",
      "test loss: 0.1613, accuracy: 0.9467\n",
      "epoch: 228\n",
      "train loss: 0.1319, accuracy: 0.9533\n",
      "test loss: 0.1616, accuracy: 0.9467\n",
      "epoch: 229\n",
      "train loss: 0.1301, accuracy: 0.9600\n",
      "test loss: 0.1640, accuracy: 0.9300\n",
      "epoch: 230\n",
      "train loss: 0.1298, accuracy: 0.9467\n",
      "test loss: 0.1656, accuracy: 0.9333\n",
      "epoch: 231\n",
      "train loss: 0.1289, accuracy: 0.9567\n",
      "test loss: 0.1633, accuracy: 0.9500\n",
      "epoch: 232\n",
      "train loss: 0.1276, accuracy: 0.9600\n",
      "test loss: 0.1632, accuracy: 0.9433\n",
      "epoch: 233\n",
      "train loss: 0.1269, accuracy: 0.9567\n",
      "test loss: 0.1675, accuracy: 0.9333\n",
      "epoch: 234\n",
      "train loss: 0.1311, accuracy: 0.9600\n",
      "test loss: 0.1630, accuracy: 0.9433\n",
      "epoch: 235\n",
      "train loss: 0.1335, accuracy: 0.9500\n",
      "test loss: 0.1602, accuracy: 0.9467\n",
      "epoch: 236\n",
      "train loss: 0.1275, accuracy: 0.9633\n",
      "test loss: 0.1596, accuracy: 0.9500\n",
      "epoch: 237\n",
      "train loss: 0.1227, accuracy: 0.9633\n",
      "test loss: 0.1619, accuracy: 0.9467\n",
      "epoch: 238\n",
      "train loss: 0.1275, accuracy: 0.9600\n",
      "test loss: 0.1588, accuracy: 0.9433\n",
      "epoch: 239\n",
      "train loss: 0.1270, accuracy: 0.9533\n",
      "test loss: 0.1657, accuracy: 0.9367\n",
      "epoch: 240\n",
      "train loss: 0.1251, accuracy: 0.9500\n",
      "test loss: 0.1583, accuracy: 0.9433\n",
      "epoch: 241\n",
      "train loss: 0.1273, accuracy: 0.9533\n",
      "test loss: 0.1589, accuracy: 0.9500\n",
      "epoch: 242\n",
      "train loss: 0.1268, accuracy: 0.9567\n",
      "test loss: 0.1584, accuracy: 0.9500\n",
      "epoch: 243\n",
      "train loss: 0.1242, accuracy: 0.9633\n",
      "test loss: 0.1580, accuracy: 0.9433\n",
      "epoch: 244\n",
      "train loss: 0.1232, accuracy: 0.9633\n",
      "test loss: 0.1642, accuracy: 0.9433\n",
      "epoch: 245\n",
      "train loss: 0.1247, accuracy: 0.9600\n",
      "test loss: 0.1562, accuracy: 0.9500\n",
      "epoch: 246\n",
      "train loss: 0.1273, accuracy: 0.9567\n",
      "test loss: 0.1555, accuracy: 0.9500\n",
      "epoch: 247\n",
      "train loss: 0.1305, accuracy: 0.9500\n",
      "test loss: 0.1552, accuracy: 0.9533\n",
      "epoch: 248\n",
      "train loss: 0.1274, accuracy: 0.9500\n",
      "test loss: 0.1579, accuracy: 0.9467\n",
      "epoch: 249\n",
      "train loss: 0.1282, accuracy: 0.9500\n",
      "test loss: 0.1589, accuracy: 0.9500\n",
      "epoch: 250\n",
      "train loss: 0.1286, accuracy: 0.9600\n",
      "test loss: 0.1548, accuracy: 0.9533\n",
      "epoch: 251\n",
      "train loss: 0.1251, accuracy: 0.9667\n",
      "test loss: 0.1566, accuracy: 0.9400\n",
      "epoch: 252\n",
      "train loss: 0.1213, accuracy: 0.9533\n",
      "test loss: 0.1615, accuracy: 0.9167\n",
      "epoch: 253\n",
      "train loss: 0.1232, accuracy: 0.9633\n",
      "test loss: 0.1588, accuracy: 0.9433\n",
      "epoch: 254\n",
      "train loss: 0.1238, accuracy: 0.9567\n",
      "test loss: 0.1564, accuracy: 0.9400\n",
      "epoch: 255\n",
      "train loss: 0.1212, accuracy: 0.9633\n",
      "test loss: 0.1571, accuracy: 0.9533\n",
      "epoch: 256\n",
      "train loss: 0.1261, accuracy: 0.9600\n",
      "test loss: 0.1546, accuracy: 0.9433\n",
      "epoch: 257\n",
      "train loss: 0.1238, accuracy: 0.9600\n",
      "test loss: 0.1575, accuracy: 0.9533\n",
      "epoch: 258\n",
      "train loss: 0.1224, accuracy: 0.9633\n",
      "test loss: 0.1583, accuracy: 0.9333\n",
      "epoch: 259\n",
      "train loss: 0.1239, accuracy: 0.9533\n",
      "test loss: 0.1534, accuracy: 0.9467\n",
      "epoch: 260\n",
      "train loss: 0.1288, accuracy: 0.9567\n",
      "test loss: 0.1522, accuracy: 0.9500\n",
      "epoch: 261\n",
      "train loss: 0.1245, accuracy: 0.9567\n",
      "test loss: 0.1530, accuracy: 0.9500\n",
      "epoch: 262\n",
      "train loss: 0.1225, accuracy: 0.9600\n",
      "test loss: 0.1549, accuracy: 0.9433\n",
      "epoch: 263\n",
      "train loss: 0.1193, accuracy: 0.9533\n",
      "test loss: 0.1523, accuracy: 0.9533\n",
      "epoch: 264\n",
      "train loss: 0.1177, accuracy: 0.9633\n",
      "test loss: 0.1553, accuracy: 0.9433\n",
      "epoch: 265\n",
      "train loss: 0.1140, accuracy: 0.9633\n",
      "test loss: 0.1566, accuracy: 0.9500\n",
      "epoch: 266\n",
      "train loss: 0.1210, accuracy: 0.9500\n",
      "test loss: 0.1507, accuracy: 0.9533\n",
      "epoch: 267\n",
      "train loss: 0.1224, accuracy: 0.9567\n",
      "test loss: 0.1511, accuracy: 0.9500\n",
      "epoch: 268\n",
      "train loss: 0.1234, accuracy: 0.9500\n",
      "test loss: 0.1504, accuracy: 0.9533\n",
      "epoch: 269\n",
      "train loss: 0.1163, accuracy: 0.9567\n",
      "test loss: 0.1511, accuracy: 0.9500\n",
      "epoch: 270\n",
      "train loss: 0.1191, accuracy: 0.9567\n",
      "test loss: 0.1507, accuracy: 0.9533\n",
      "epoch: 271\n",
      "train loss: 0.1187, accuracy: 0.9567\n",
      "test loss: 0.1543, accuracy: 0.9367\n",
      "epoch: 272\n",
      "train loss: 0.1182, accuracy: 0.9633\n",
      "test loss: 0.1567, accuracy: 0.9267\n",
      "epoch: 273\n",
      "train loss: 0.1137, accuracy: 0.9567\n",
      "test loss: 0.1490, accuracy: 0.9533\n",
      "epoch: 274\n",
      "train loss: 0.1205, accuracy: 0.9467\n",
      "test loss: 0.1541, accuracy: 0.9400\n",
      "epoch: 275\n",
      "train loss: 0.1159, accuracy: 0.9633\n",
      "test loss: 0.1508, accuracy: 0.9467\n",
      "epoch: 276\n",
      "train loss: 0.1151, accuracy: 0.9633\n",
      "test loss: 0.1518, accuracy: 0.9533\n",
      "epoch: 277\n",
      "train loss: 0.1166, accuracy: 0.9633\n",
      "test loss: 0.1578, accuracy: 0.9233\n",
      "epoch: 278\n",
      "train loss: 0.1172, accuracy: 0.9667\n",
      "test loss: 0.1548, accuracy: 0.9400\n",
      "epoch: 279\n",
      "train loss: 0.1134, accuracy: 0.9633\n",
      "test loss: 0.1499, accuracy: 0.9533\n",
      "epoch: 280\n",
      "train loss: 0.1162, accuracy: 0.9533\n",
      "test loss: 0.1506, accuracy: 0.9500\n",
      "epoch: 281\n",
      "train loss: 0.1189, accuracy: 0.9700\n",
      "test loss: 0.1478, accuracy: 0.9567\n",
      "epoch: 282\n",
      "train loss: 0.1188, accuracy: 0.9600\n",
      "test loss: 0.1484, accuracy: 0.9533\n",
      "epoch: 283\n",
      "train loss: 0.1142, accuracy: 0.9667\n",
      "test loss: 0.1524, accuracy: 0.9433\n",
      "epoch: 284\n",
      "train loss: 0.1123, accuracy: 0.9700\n",
      "test loss: 0.1500, accuracy: 0.9433\n",
      "epoch: 285\n",
      "train loss: 0.1192, accuracy: 0.9600\n",
      "test loss: 0.1508, accuracy: 0.9467\n",
      "epoch: 286\n",
      "train loss: 0.1114, accuracy: 0.9600\n",
      "test loss: 0.1471, accuracy: 0.9533\n",
      "epoch: 287\n",
      "train loss: 0.1109, accuracy: 0.9633\n",
      "test loss: 0.1481, accuracy: 0.9567\n",
      "epoch: 288\n",
      "train loss: 0.1143, accuracy: 0.9533\n",
      "test loss: 0.1534, accuracy: 0.9467\n",
      "epoch: 289\n",
      "train loss: 0.1126, accuracy: 0.9600\n",
      "test loss: 0.1486, accuracy: 0.9500\n",
      "epoch: 290\n",
      "train loss: 0.1125, accuracy: 0.9733\n",
      "test loss: 0.1485, accuracy: 0.9533\n",
      "epoch: 291\n",
      "train loss: 0.1136, accuracy: 0.9700\n",
      "test loss: 0.1472, accuracy: 0.9500\n",
      "epoch: 292\n",
      "train loss: 0.1130, accuracy: 0.9600\n",
      "test loss: 0.1495, accuracy: 0.9433\n",
      "epoch: 293\n",
      "train loss: 0.1118, accuracy: 0.9700\n",
      "test loss: 0.1456, accuracy: 0.9533\n",
      "epoch: 294\n",
      "train loss: 0.1111, accuracy: 0.9600\n",
      "test loss: 0.1591, accuracy: 0.9367\n",
      "epoch: 295\n",
      "train loss: 0.1170, accuracy: 0.9533\n",
      "test loss: 0.1451, accuracy: 0.9533\n",
      "epoch: 296\n",
      "train loss: 0.1115, accuracy: 0.9567\n",
      "test loss: 0.1459, accuracy: 0.9567\n",
      "epoch: 297\n",
      "train loss: 0.1096, accuracy: 0.9667\n",
      "test loss: 0.1464, accuracy: 0.9533\n",
      "epoch: 298\n",
      "train loss: 0.1128, accuracy: 0.9500\n",
      "test loss: 0.1476, accuracy: 0.9533\n",
      "epoch: 299\n",
      "train loss: 0.1090, accuracy: 0.9633\n",
      "test loss: 0.1471, accuracy: 0.9467\n",
      "epoch: 300\n",
      "train loss: 0.1082, accuracy: 0.9667\n",
      "test loss: 0.1458, accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = datasets.Spiral(train=True)\n",
    "test_set = datasets.Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch + 1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)\n",
    "    ))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with kdezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading: t10k-images-idx3-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "Downloading: t10k-labels-idx1-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "train_set = kdezero.datasets.MNIST(train=True, transform=None)\n",
    "test_set = kdezero.datasets.MNIST(train=False, transform=None)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'> (1, 28, 28)\n5\n"
     ]
    }
   ],
   "source": [
    "x, t = train_set[0]\n",
    "print(type(x), x.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 231.84 231.84\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-03-31T11:40:35.533123</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 231.84 \r\nL 231.84 231.84 \r\nL 231.84 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g clip-path=\"url(#p9e99ae24d1)\">\r\n    <image height=\"218\" id=\"imaged0fbe68b87\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p9e99ae24d1\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "label: 5\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('label:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1\n",
      "train loss: 1.9280, accuracy: 0.5447\n",
      "test loss: 1.5457, accuracy: 0.7461\n",
      "epoch: 2\n",
      "train loss: 1.2896, accuracy: 0.7677\n",
      "test loss: 1.0481, accuracy: 0.8080\n",
      "epoch: 3\n",
      "train loss: 0.9264, accuracy: 0.8190\n",
      "test loss: 0.7927, accuracy: 0.8446\n",
      "epoch: 4\n",
      "train loss: 0.7409, accuracy: 0.8403\n",
      "test loss: 0.6591, accuracy: 0.8524\n",
      "epoch: 5\n",
      "train loss: 0.6357, accuracy: 0.8534\n",
      "test loss: 0.5770, accuracy: 0.8639\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set = kdezero.datasets.MNIST(train=True)\n",
    "test_set = kdezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch + 1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)\n",
    "    ))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with kdezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1\n",
      "train loss: 0.1912, accuracy: 0.9421\n",
      "test loss: 0.0956, accuracy: 0.9692\n",
      "epoch: 2\n",
      "train loss: 0.0742, accuracy: 0.9763\n",
      "test loss: 0.0838, accuracy: 0.9730\n",
      "epoch: 3\n",
      "train loss: 0.0473, accuracy: 0.9848\n",
      "test loss: 0.0740, accuracy: 0.9773\n",
      "epoch: 4\n",
      "train loss: 0.0344, accuracy: 0.9889\n",
      "test loss: 0.0800, accuracy: 0.9769\n",
      "epoch: 5\n",
      "train loss: 0.0293, accuracy: 0.9901\n",
      "test loss: 0.0713, accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set = kdezero.datasets.MNIST(train=True)\n",
    "test_set = kdezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, hidden_size, 10), activation=F.relu)\n",
    "optimizer = optimizers.Adam().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch + 1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)\n",
    "    ))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with kdezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}